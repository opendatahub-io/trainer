{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Enhanced TRL Training with Kubeflow SDK and Advanced Checkpointing\n",
        "\n",
        "This notebook demonstrates how to use the **Kubeflow Trainer SDK** to create and manage TrainJobs with enhanced checkpointing capabilities, following the same pattern as `trl-trainjob-clean.yaml`.\n",
        "\n",
        "## üéØ Features Demonstrated\n",
        "\n",
        "- ‚úÖ **Kubeflow SDK Integration**: Programmatic TrainJob creation and management\n",
        "- ‚úÖ **Enhanced Checkpointing**: Controller-managed progress tracking and model checkpoints\n",
        "- ‚úÖ **TRL SFTTrainer**: Advanced supervised fine-tuning with LoRA\n",
        "- ‚úÖ **Distributed Training**: Multi-node coordination with operator-injected variables\n",
        "- ‚úÖ **Real-Time Monitoring**: Live progress tracking via Kubernetes API\n",
        "- ‚úÖ **Production Ready**: Fault tolerance and graceful shutdown handling\n",
        "\n",
        "## üìö References\n",
        "- [Kubeflow Trainer SDK](https://github.com/kubeflow/sdk)\n",
        "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Kubeflow SDK from local development version\n",
        "%pip install /Users/abdhumal/Dev/RedHatDev/sdk/dist/kubeflow-0.1.0-py3-none-any.whl\n",
        "\n",
        "# Install additional dependencies for training\n",
        "%pip install \"transformers[torch]\" \"trl\" \"peft\" \"datasets\" \"accelerate\" \"torch\" \"numpy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üîß Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from kubeflow.trainer import TrainerClient, CustomTrainer, HuggingFaceDatasetInitializer, HuggingFaceModelInitializer, Initializer\n",
        "\n",
        "# Initialize Kubeflow TrainerClient\n",
        "trainer_client = TrainerClient(namespace=\"abdhumal-test\")\n",
        "\n",
        "# Configuration\n",
        "NAMESPACE = \"abdhumal-test\"\n",
        "print(f\"üéÆ Kubeflow TrainerClient initialized for namespace: {NAMESPACE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üêç Load Exact Training Script from trl-trainjob.yaml\n",
        "\n",
        "This cell extracts and loads the **identical** training script embedded in `trl-trainjob.yaml`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the TRL training function that matches trl-trainjob.yaml\n",
        "def train_gpt2_with_trl_checkpointing(args):\n",
        "    \"\"\"\n",
        "    Advanced TRL training script with controller integration and distributed coordination.\n",
        "    This matches the exact same training logic as embedded in trl-trainjob.yaml.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import time\n",
        "    import signal\n",
        "    import torch\n",
        "    import random\n",
        "    import numpy\n",
        "    from numpy.core.multiarray import _reconstruct\n",
        "    import torch.serialization\n",
        "    import torch.distributed as dist\n",
        "    import logging\n",
        "    from datetime import datetime\n",
        "    from pathlib import Path\n",
        "    from datasets import load_dataset, load_from_disk\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        TrainingArguments,\n",
        "        TrainerState,\n",
        "        TrainerControl,\n",
        "        TrainerCallback,\n",
        "        set_seed,\n",
        "    )\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "    from trl import (\n",
        "        ModelConfig,\n",
        "        ScriptArguments,\n",
        "        SFTConfig,\n",
        "        SFTTrainer,\n",
        "        TrlParser,\n",
        "        get_peft_config,\n",
        "        get_quantization_config,\n",
        "        get_kbit_device_map,\n",
        "    )\n",
        "    \n",
        "    # Safe tensor loading configuration\n",
        "    torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])\n",
        "    \n",
        "    # Patch torch.load to handle weights_only parameter and device mapping\n",
        "    original_torch_load = torch.load\n",
        "    def patched_torch_load(*args, **kwargs):\n",
        "        if 'weights_only' not in kwargs:\n",
        "            kwargs['weights_only'] = False\n",
        "        if 'map_location' not in kwargs:\n",
        "            if torch.cuda.is_available():\n",
        "                kwargs['map_location'] = 'cuda'\n",
        "            else:\n",
        "                kwargs['map_location'] = 'cpu'\n",
        "        return original_torch_load(*args, **kwargs)\n",
        "    torch.load = patched_torch_load\n",
        "    \n",
        "    class AdvancedDistributedCheckpointCallback(TrainerCallback):\n",
        "        \"\"\"\n",
        "        Production-grade distributed SIGTERM handling with tensor-based coordination.\n",
        "        Combines the example's advanced features with controller integration.\n",
        "        \"\"\"\n",
        "        def __init__(self, output_dir: str):\n",
        "            self.output_dir = output_dir\n",
        "            self.checkpoint_requested = False\n",
        "            self.save_triggered = False\n",
        "            self.checkpoint_stream = None\n",
        "            self.sigterm_tensor = None\n",
        "            \n",
        "            # Use controller-injected checkpoint configuration\n",
        "            self.checkpoint_enabled = os.environ.get('CHECKPOINT_ENABLED', 'false').lower() == 'true'\n",
        "            self.checkpoint_uri = os.environ.get('CHECKPOINT_URI', '/workspace/checkpoints')\n",
        "            \n",
        "            # Controller progress file (simple)\n",
        "            self.progress_file = os.environ.get('TRAINING_PROGRESS_FILE', '/workspace/training_progress.json')\n",
        "\n",
        "        def _log_message(self, message: str):\n",
        "            \"\"\"Helper to print messages with a timestamp.\"\"\"\n",
        "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            print(f\"[{timestamp}] {message}\")\n",
        "        \n",
        "        def _write_progress(self, state: TrainerState):\n",
        "            \"\"\"Simple progress file writer for controller consumption\"\"\"\n",
        "            # Only write from rank 0\n",
        "            rank = int(os.environ.get('RANK', '0'))\n",
        "            if rank != 0:\n",
        "                return\n",
        "                \n",
        "            try:\n",
        "                # Extract metrics from trainer state\n",
        "                latest_loss = 0.0\n",
        "                latest_lr = 0.0\n",
        "                if state.log_history:\n",
        "                    latest_log = state.log_history[-1]\n",
        "                    latest_loss = latest_log.get('loss', latest_log.get('train_loss', latest_log.get('training_loss', 0.0)))\n",
        "                    latest_lr = latest_log.get('learning_rate', latest_log.get('lr', latest_log.get('train_lr', 0.0)))\n",
        "                \n",
        "                progress_data = {\n",
        "                    \"epoch\": int(state.epoch) if state.epoch else 1,\n",
        "                    \"totalEpochs\": int(state.num_train_epochs) if state.num_train_epochs else 1,\n",
        "                    \"step\": state.global_step,\n",
        "                    \"totalSteps\": state.max_steps,\n",
        "                    \"loss\": f\"{latest_loss:.4f}\",\n",
        "                    \"learningRate\": f\"{latest_lr:.6f}\",\n",
        "                    \"percentComplete\": f\"{(state.global_step / state.max_steps * 100):.1f}\" if state.max_steps > 0 else \"0.0\",\n",
        "                    \"lastUpdateTime\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "                }\n",
        "                \n",
        "                # Atomic write\n",
        "                temp_file = self.progress_file + '.tmp'\n",
        "                with open(temp_file, 'w') as f:\n",
        "                    json.dump(progress_data, f, indent=2)\n",
        "                os.rename(temp_file, self.progress_file)\n",
        "                os.chmod(self.progress_file, 0o644)\n",
        "                \n",
        "            except Exception as e:\n",
        "                pass  # Silent fail - controller handles missing files gracefully\n",
        "\n",
        "        def _init_distributed_signal_tensor(self):\n",
        "            \"\"\"Initialize tensor for distributed SIGTERM signaling.\"\"\"\n",
        "            try:\n",
        "                if dist.is_initialized():\n",
        "                    device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
        "                    self.sigterm_tensor = torch.zeros(1, dtype=torch.float32, device=device)\n",
        "                    self._log_message(f\"Initialized distributed SIGTERM tensor on device: {device}\")\n",
        "                else:\n",
        "                    self._log_message(\"Distributed training not initialized - using local SIGTERM handling only\")\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Failed to initialize distributed SIGTERM tensor: {e}. Using local handling only.\")\n",
        "\n",
        "        def _check_distributed_sigterm(self):\n",
        "            \"\"\"Check if any rank has received SIGTERM.\"\"\"\n",
        "            try:\n",
        "                if dist.is_initialized() and self.sigterm_tensor is not None:\n",
        "                    dist.all_reduce(self.sigterm_tensor, op=dist.ReduceOp.MAX)\n",
        "                    return self.sigterm_tensor.item() > 0.5\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Distributed SIGTERM check failed: {e}. Using local signal only.\")\n",
        "            return self.checkpoint_requested\n",
        "\n",
        "        def _sigterm_handler(self, signum, frame):\n",
        "            \"\"\"Sets a flag and updates the tensor to indicate that a SIGTERM signal was received.\"\"\"\n",
        "            rank = os.environ.get(\"RANK\", \"-1\")\n",
        "            self._log_message(f\"Rank {rank}: SIGTERM received, flagging for distributed checkpoint.\")\n",
        "            self.checkpoint_requested = True\n",
        "            if self.sigterm_tensor is not None:\n",
        "                self.sigterm_tensor.fill_(1.0)\n",
        "\n",
        "        def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            rank = os.environ.get(\"RANK\", \"-1\")\n",
        "            os.makedirs(self.output_dir, exist_ok=True)\n",
        "            self._init_distributed_signal_tensor()\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                self.checkpoint_stream = torch.cuda.Stream()\n",
        "                self._log_message(f\"Rank {rank}: Created dedicated CUDA stream for checkpointing.\")\n",
        "\n",
        "            signal.signal(signal.SIGTERM, self._sigterm_handler)\n",
        "            self._log_message(f\"Rank {rank}: Advanced distributed SIGTERM handler registered.\")\n",
        "\n",
        "            try:\n",
        "                if dist.is_initialized():\n",
        "                    dist.barrier()\n",
        "                    self._log_message(f\"Rank {rank}: Distributed coordination setup synchronized across all ranks\")\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Rank {rank}: Failed to synchronize distributed setup: {e}\")\n",
        "\n",
        "        def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            # Write progress for controller (every logging step)\n",
        "            if state.global_step % args.logging_steps == 0:\n",
        "                self._write_progress(state)\n",
        "                \n",
        "            if self._check_distributed_sigterm() and not self.save_triggered:\n",
        "                rank = os.environ.get(\"RANK\", \"-1\")\n",
        "                self._log_message(f\"Rank {rank}: Distributed SIGTERM detected, initiating checkpoint at step {state.global_step}.\")\n",
        "                self.save_triggered = True\n",
        "                control.should_save = True\n",
        "                control.should_training_stop = True\n",
        "\n",
        "        def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            # Write final progress\n",
        "            self._write_progress(state)\n",
        "            \n",
        "            rank = os.environ.get(\"RANK\", \"-1\")\n",
        "            if rank == \"0\" and self.checkpoint_requested:\n",
        "                self._log_message(f\"Rank {rank}: Training ended due to distributed SIGTERM checkpoint request.\")\n",
        "\n",
        "        def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            rank = os.environ.get(\"RANK\", \"-1\")\n",
        "            if rank == \"0\":\n",
        "                self._log_message(f\"Rank {rank}: Checkpoint save completed.\")\n",
        "                if self.checkpoint_requested:\n",
        "                    self._log_message(f\"Rank {rank}: Distributed SIGTERM-triggered checkpoint save finished successfully.\")\n",
        "    \n",
        "    def setup_distributed():\n",
        "        \"\"\"Initialize distributed training using operator-injected PET environment variables\"\"\"\n",
        "        # Use PET_* environment variables injected by the training operator\n",
        "        node_rank = int(os.getenv('PET_NODE_RANK', '0'))\n",
        "        num_nodes = int(os.getenv('PET_NNODES', '1'))\n",
        "        nproc_per_node = int(os.getenv('PET_NPROC_PER_NODE', '1'))\n",
        "        master_addr = os.getenv('PET_MASTER_ADDR', 'localhost')\n",
        "        master_port = os.getenv('PET_MASTER_PORT', '29500')\n",
        "        \n",
        "        # Calculate standard PyTorch distributed variables\n",
        "        local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
        "        world_size = num_nodes * nproc_per_node\n",
        "        global_rank = node_rank * nproc_per_node + local_rank\n",
        "        \n",
        "        # Set standard PyTorch environment variables for compatibility\n",
        "        os.environ['RANK'] = str(global_rank)\n",
        "        os.environ['WORLD_SIZE'] = str(world_size)\n",
        "        os.environ['LOCAL_RANK'] = str(local_rank)\n",
        "        os.environ['MASTER_ADDR'] = master_addr\n",
        "        os.environ['MASTER_PORT'] = master_port\n",
        "        \n",
        "        # Initialize distributed training if world_size > 1\n",
        "        if world_size > 1:\n",
        "            try:\n",
        "                torch.distributed.init_process_group(\n",
        "                    backend='gloo',\n",
        "                    rank=global_rank,\n",
        "                    world_size=world_size\n",
        "                )\n",
        "                torch.distributed.barrier()\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to initialize distributed training: {e}\")\n",
        "        \n",
        "        return local_rank, global_rank, world_size\n",
        "    \n",
        "    def load_dataset_from_initializer():\n",
        "        \"\"\"Load dataset from V2 initializer or fallback to download\"\"\"\n",
        "        dataset_dir = Path(\"/workspace/dataset\")\n",
        "        \n",
        "        if dataset_dir.exists() and any(dataset_dir.iterdir()):\n",
        "            try:\n",
        "                full_dataset = load_from_disk(str(dataset_dir))\n",
        "                if isinstance(full_dataset, dict):\n",
        "                    train_dataset = full_dataset.get('train', full_dataset.get('train[:100]'))\n",
        "                    test_dataset = full_dataset.get('test', full_dataset.get('test[:20]'))\n",
        "                else:\n",
        "                    # Split dataset if it's not already split\n",
        "                    train_size = min(100, len(full_dataset) - 20)\n",
        "                    train_dataset = full_dataset.select(range(train_size))\n",
        "                    test_dataset = full_dataset.select(range(train_size, min(train_size + 20, len(full_dataset))))\n",
        "                \n",
        "                return train_dataset, test_dataset\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load from initializer: {e}\")\n",
        "        \n",
        "        # Fallback to direct download\n",
        "        dataset_name = os.getenv('DATASET_NAME', 'tatsu-lab/alpaca')\n",
        "        train_split = os.getenv('DATASET_TRAIN_SPLIT', 'train[:100]')\n",
        "        test_split = os.getenv('DATASET_TEST_SPLIT', 'train[100:120]')\n",
        "        \n",
        "        train_dataset = load_dataset(dataset_name, split=train_split)\n",
        "        test_dataset = load_dataset(dataset_name, split=test_split)\n",
        "        \n",
        "        return train_dataset, test_dataset\n",
        "    \n",
        "    def load_model_from_initializer():\n",
        "        \"\"\"Load model and tokenizer from V2 initializer or fallback to download\"\"\"\n",
        "        model_dir = Path(\"/workspace/model\")\n",
        "        \n",
        "        if model_dir.exists() and any(model_dir.iterdir()):\n",
        "            model_path = str(model_dir)\n",
        "        else:\n",
        "            model_path = os.getenv('MODEL_NAME', 'gpt2')\n",
        "        \n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=True)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            \n",
        "            # Set up chat template for instruction following\n",
        "            if tokenizer.chat_template is None:\n",
        "                tokenizer.chat_template = (\n",
        "                    \"{% for message in messages %}\"\n",
        "                    \"{% if message['role'] == 'user' %}\"\n",
        "                    \"### Instruction:\\n{{ message['content'] }}\\n\"\n",
        "                    \"{% elif message['role'] == 'assistant' %}\"\n",
        "                    \"### Response:\\n{{ message['content'] }}{{ eos_token }}\\n\"\n",
        "                    \"{% endif %}\"\n",
        "                    \"{% endfor %}\"\n",
        "                )\n",
        "            \n",
        "            return model_path, tokenizer\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            # Fallback to gpt2\n",
        "            model_path = 'gpt2'\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            return model_path, tokenizer\n",
        "    \n",
        "    def prepare_datasets(train_dataset, test_dataset, tokenizer):\n",
        "        \"\"\"Prepare datasets for training with proper formatting\"\"\"\n",
        "        def template_dataset(sample):\n",
        "            # Handle different dataset formats\n",
        "            if 'instruction' in sample and 'output' in sample:\n",
        "                # Alpaca format\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": sample['instruction']},\n",
        "                    {\"role\": \"assistant\", \"content\": sample['output']},\n",
        "                ]\n",
        "            elif 'question' in sample and 'answer' in sample:\n",
        "                # GSM8K format\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": sample['question']},\n",
        "                    {\"role\": \"assistant\", \"content\": sample['answer']},\n",
        "                ]\n",
        "            else:\n",
        "                # Fallback format\n",
        "                content = str(sample.get('text', sample.get('content', 'Sample text')))\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": \"Complete this text:\"},\n",
        "                    {\"role\": \"assistant\", \"content\": content},\n",
        "                ]\n",
        "            \n",
        "            return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "        \n",
        "        # Get original column names to remove\n",
        "        train_columns = list(train_dataset.features.keys())\n",
        "        train_columns.remove('text') if 'text' in train_columns else None\n",
        "        \n",
        "        train_dataset = train_dataset.map(template_dataset, remove_columns=train_columns)\n",
        "        \n",
        "        if test_dataset is not None:\n",
        "            test_columns = list(test_dataset.features.keys())\n",
        "            test_columns.remove('text') if 'text' in test_columns else None\n",
        "            test_dataset = test_dataset.map(template_dataset, remove_columns=test_columns)\n",
        "        \n",
        "        return train_dataset, test_dataset\n",
        "    \n",
        "    def get_training_parameters():\n",
        "        \"\"\"Get training parameters from controller and environment variables\"\"\"\n",
        "        # Use controller-injected checkpoint configuration\n",
        "        checkpoint_dir = Path(os.getenv('CHECKPOINT_URI', '/workspace/checkpoints'))\n",
        "        checkpoint_enabled = os.getenv('CHECKPOINT_ENABLED', 'false').lower() == 'true'\n",
        "        checkpoint_interval = os.getenv('CHECKPOINT_INTERVAL', '30s')\n",
        "        max_checkpoints = int(os.getenv('CHECKPOINT_MAX_RETAIN', '5'))\n",
        "        \n",
        "        # Training hyperparameters from environment (with sensible defaults)\n",
        "        parameters = {\n",
        "            'model_name_or_path': os.getenv('MODEL_NAME', 'gpt2'),\n",
        "            'model_revision': 'main',\n",
        "            'torch_dtype': 'bfloat16',\n",
        "            'use_peft': True,\n",
        "            'lora_r': int(os.getenv('LORA_R', '16')),\n",
        "            'lora_alpha': int(os.getenv('LORA_ALPHA', '32')),\n",
        "            'lora_dropout': float(os.getenv('LORA_DROPOUT', '0.1')),\n",
        "            'lora_target_modules': ['c_attn', 'c_proj'],  # GPT-2 specific\n",
        "            'dataset_name': os.getenv('DATASET_NAME', 'tatsu-lab/alpaca'),\n",
        "            'dataset_config': 'main',\n",
        "            'dataset_train_split': os.getenv('DATASET_TRAIN_SPLIT', 'train[:100]'),\n",
        "            'dataset_test_split': os.getenv('DATASET_TEST_SPLIT', 'train[100:120]'),\n",
        "            'max_seq_length': int(os.getenv('MAX_SEQ_LENGTH', '512')),\n",
        "            'num_train_epochs': int(os.getenv('MAX_EPOCHS', '3')),\n",
        "            'per_device_train_batch_size': int(os.getenv('BATCH_SIZE', '2')),\n",
        "            'per_device_eval_batch_size': int(os.getenv('BATCH_SIZE', '2')),\n",
        "            'eval_strategy': 'steps',\n",
        "            'eval_steps': int(os.getenv('EVAL_STEPS', '25')),\n",
        "            'bf16': torch.cuda.is_available(),  # Only use bf16 if CUDA is available\n",
        "            'fp16': not torch.cuda.is_available(),  # Use fp16 for CPU training\n",
        "            'learning_rate': float(os.getenv('LEARNING_RATE', '5e-5')),\n",
        "            'warmup_steps': int(os.getenv('WARMUP_STEPS', '10')),\n",
        "            'lr_scheduler_type': 'cosine',\n",
        "            'optim': 'adamw_torch',\n",
        "            'max_grad_norm': 1.0,\n",
        "            'seed': 42,\n",
        "            'gradient_accumulation_steps': int(os.getenv('GRADIENT_ACCUMULATION_STEPS', '4')),\n",
        "            'save_strategy': 'steps',\n",
        "            'save_steps': int(os.getenv('SAVE_STEPS', '20')),\n",
        "            'save_total_limit': max_checkpoints if checkpoint_enabled else None,\n",
        "            'logging_strategy': 'steps',\n",
        "            'logging_steps': int(os.getenv('LOGGING_STEPS', '5')),\n",
        "            'report_to': [],\n",
        "            'output_dir': str(checkpoint_dir),\n",
        "        }\n",
        "        \n",
        "        return parameters\n",
        "    \n",
        "    # Main training function with controller integration and distributed coordination\n",
        "    import os\n",
        "    \n",
        "    # Setup distributed training\n",
        "    local_rank, global_rank, world_size = setup_distributed()\n",
        "    \n",
        "    # Create necessary directories\n",
        "    os.makedirs(\"/workspace/cache/transformers\", exist_ok=True)\n",
        "    os.makedirs(\"/workspace/cache\", exist_ok=True)\n",
        "    os.makedirs(\"/workspace/cache/datasets\", exist_ok=True)\n",
        "    \n",
        "    # Get training parameters\n",
        "    parameters = get_training_parameters()\n",
        "    checkpoint_dir = Path(parameters['output_dir'])\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    # Parse configuration using TrlParser for robust handling\n",
        "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
        "    script_args, training_args, model_args = parser.parse_dict(parameters)\n",
        "    \n",
        "    set_seed(training_args.seed)\n",
        "    \n",
        "    # Load components using V2 initializers\n",
        "    model_path, tokenizer = load_model_from_initializer()\n",
        "    train_dataset, test_dataset = load_dataset_from_initializer()\n",
        "    train_dataset, test_dataset = prepare_datasets(train_dataset, test_dataset, tokenizer)\n",
        "    \n",
        "    # Initialize trainer with advanced callbacks\n",
        "    callbacks = [\n",
        "        AdvancedDistributedCheckpointCallback(str(checkpoint_dir))  # Advanced distributed coordination\n",
        "    ]\n",
        "    \n",
        "    # Initialize SFTTrainer with controller integration\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_path,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        peft_config=get_peft_config(model_args),\n",
        "        processing_class=tokenizer,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    \n",
        "    # Print trainable parameters info\n",
        "    if trainer.accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
        "        trainer.model.print_trainable_parameters()\n",
        "    \n",
        "    # Check for resume from checkpoint (controller-managed)\n",
        "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "    if checkpoint is not None:\n",
        "        # Validate checkpoint compatibility before resuming\n",
        "        try:\n",
        "            checkpoint_files = os.listdir(checkpoint)\n",
        "            # Check if this is a valid checkpoint directory\n",
        "            if 'trainer_state.json' not in checkpoint_files:\n",
        "                checkpoint = None\n",
        "        except Exception as e:\n",
        "            print(f\"Checkpoint validation failed: {e}\")\n",
        "            checkpoint = None\n",
        "    \n",
        "    # Start training\n",
        "    try:\n",
        "        trainer.train(resume_from_checkpoint=checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {e}\")\n",
        "        # If checkpoint loading failed, try without checkpoint\n",
        "        if checkpoint is not None:\n",
        "            try:\n",
        "                trainer.train(resume_from_checkpoint=None)\n",
        "            except Exception as retry_e:\n",
        "                print(f\"Training failed even from scratch: {retry_e}\")\n",
        "                raise retry_e\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    # Save final model\n",
        "    trainer.save_model(training_args.output_dir)\n",
        "\n",
        "print(\"‚úÖ TRL training function defined (matches trl-trainjob.yaml logic)\")\n",
        "print(\"üìã Available function: train_gpt2_with_trl_checkpointing(args)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üêç Define TRL Training Function (Identical to trl-trainjob.yaml)\n",
        "\n",
        "This function implements the **exact same** advanced training logic as embedded in `trl-trainjob.yaml` with:\n",
        "- Minimal progress file writer (no redundant callbacks)\n",
        "- Advanced distributed checkpoint coordination\n",
        "- Controller-managed progress tracking\n",
        "- Automated model checkpointing by SIGTERM signal handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the exact training script from trl-trainjob.yaml\n",
        "import subprocess\n",
        "import tempfile\n",
        "\n",
        "def extract_training_script_from_yaml():\n",
        "    \"\"\"Extract the embedded Python script from trl-trainjob.yaml\"\"\"\n",
        "    cmd = [\"awk\", \"/advanced_trl_training.py: \\\\|/{flag=1; next} /^---$/{flag=0} flag\", \n",
        "           \"examples/checkpointing/trl-trainjob.yaml\"]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=\"/Users/abdhumal/Dev/RedHatDev/training-operator\")\n",
        "    return result.stdout\n",
        "\n",
        "# Get the exact script content\n",
        "script_content = extract_training_script_from_yaml()\n",
        "print(f\"‚úÖ Extracted {len(script_content)} characters from trl-trainjob.yaml embedded script\")\n",
        "\n",
        "# Execute the script in the current namespace\n",
        "exec(script_content)\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        TrainingArguments,\n",
        "        TrainerState,\n",
        "        TrainerControl,\n",
        "        TrainerCallback,\n",
        "        set_seed,\n",
        "    )\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "    from trl import (\n",
        "        ModelConfig,\n",
        "        ScriptArguments,\n",
        "        SFTConfig,\n",
        "        SFTTrainer,\n",
        "        TrlParser,\n",
        "        get_peft_config,\n",
        "        get_quantization_config,\n",
        "        get_kbit_device_map,\n",
        "    )\n",
        "    \n",
        "    # Safe tensor loading configuration\n",
        "    torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])\n",
        "    \n",
        "    # Patch torch.load for device compatibility\n",
        "    original_torch_load = torch.load\n",
        "    def patched_torch_load(*args, **kwargs):\n",
        "        if 'weights_only' not in kwargs:\n",
        "            kwargs['weights_only'] = False\n",
        "        if 'map_location' not in kwargs:\n",
        "            if torch.cuda.is_available():\n",
        "                kwargs['map_location'] = 'cuda'\n",
        "            else:\n",
        "                kwargs['map_location'] = 'cpu'\n",
        "        return original_torch_load(*args, **kwargs)\n",
        "    torch.load = patched_torch_load\n",
        "    \n",
        "    print(\"üöÄ Starting advanced TRL training with controller integration...\")\n",
        "    \n",
        "    class ControllerProgressCallback(TrainerCallback):\n",
        "        \"\"\"Controller-integrated progress callback for CheckpointingManager\"\"\"\n",
        "        def __init__(self):\n",
        "            self.progress_file = os.environ.get('TRAINING_PROGRESS_FILE', '/workspace/training_progress.json')\n",
        "            self.checkpoint_enabled = os.environ.get('CHECKPOINT_ENABLED', 'false').lower() == 'true'\n",
        "            print(f\"üìä Controller progress tracking: file={self.progress_file}, checkpointing={self.checkpoint_enabled}\")\n",
        "        \n",
        "        def _save_progress(self, state: TrainerState, epoch: int = None):\n",
        "            rank = int(os.environ.get('RANK', '0'))\n",
        "            if rank != 0:\n",
        "                return\n",
        "                \n",
        "            try:\n",
        "                progress_data = {\n",
        "                    \"epoch\": epoch or int(state.epoch) if state.epoch else 1,\n",
        "                    \"totalEpochs\": int(state.num_train_epochs) if state.num_train_epochs else 1,\n",
        "                    \"step\": state.global_step,\n",
        "                    \"totalSteps\": state.max_steps,\n",
        "                    \"loss\": f\"{state.log_history[-1].get('train_loss', 0.0):.4f}\" if state.log_history else \"0.0000\",\n",
        "                    \"learningRate\": f\"{state.log_history[-1].get('learning_rate', 0.0):.6f}\" if state.log_history else \"0.000000\",\n",
        "                    \"percentComplete\": f\"{(state.global_step / state.max_steps * 100):.1f}\" if state.max_steps > 0 else \"0.0\",\n",
        "                    \"lastUpdateTime\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "                }\n",
        "                \n",
        "                # Atomic write for controller consumption\n",
        "                temp_file = self.progress_file + '.tmp'\n",
        "                with open(temp_file, 'w') as f:\n",
        "                    json.dump(progress_data, f, indent=2)\n",
        "                os.rename(temp_file, self.progress_file)\n",
        "                os.chmod(self.progress_file, 0o644)\n",
        "                \n",
        "                print(f\"üìà Progress saved: step {state.global_step}/{state.max_steps} ({progress_data['percentComplete']}%)\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to save progress: {e}\")\n",
        "        \n",
        "        def on_step_end(self, args, state, control, **kwargs):\n",
        "            if state.global_step % args.logging_steps == 0:\n",
        "                self._save_progress(state)\n",
        "        \n",
        "        def on_epoch_end(self, args, state, control, **kwargs):\n",
        "            self._save_progress(state, epoch=int(state.epoch))\n",
        "    \n",
        "    # Additional implementation continues...\n",
        "    print(\"‚úÖ Advanced TRL training function defined\")\n",
        "\n",
        "print(\"‚úÖ Advanced TRL training function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Create TrainJob Using Kubeflow SDK (Matching trl-trainjob.yaml)\n",
        "\n",
        "Now we'll use the **Kubeflow SDK** to create a TrainJob that matches the same procedure as `trl-trainjob.yaml`:\n",
        "1. **CustomTrainer** with the TRL training function\n",
        "2. **Initializer** for dataset and model (V2 initializers)\n",
        "3. **TrainJob** with identical configuration and checkpointing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Configure the CustomTrainer with TRL training function\n",
        "training_args = {\n",
        "    # Training hyperparameters (matching trl-trainjob.yaml)\n",
        "    \"LEARNING_RATE\": \"5e-5\",\n",
        "    \"BATCH_SIZE\": \"1\",\n",
        "    \"MAX_EPOCHS\": \"10\",\n",
        "    \"WARMUP_STEPS\": \"5\",\n",
        "    \"EVAL_STEPS\": \"10\",\n",
        "    \"SAVE_STEPS\": \"5\",\n",
        "    \"LOGGING_STEPS\": \"2\",\n",
        "    \"GRADIENT_ACCUMULATION_STEPS\": \"2\",\n",
        "    \n",
        "    # Model configuration\n",
        "    \"MODEL_NAME\": \"gpt2\",\n",
        "    \"LORA_R\": \"16\",\n",
        "    \"LORA_ALPHA\": \"32\",\n",
        "    \"LORA_DROPOUT\": \"0.1\",\n",
        "    \"MAX_SEQ_LENGTH\": \"512\",\n",
        "    \n",
        "    # Dataset configuration\n",
        "    \"DATASET_NAME\": \"tatsu-lab/alpaca\",\n",
        "    \"DATASET_TRAIN_SPLIT\": \"train[:500]\",\n",
        "    \"DATASET_TEST_SPLIT\": \"train[500:520]\",\n",
        "    \n",
        "    # Checkpointing configuration (will be injected by controller)\n",
        "    \"CHECKPOINT_URI\": \"/workspace/checkpoints\",\n",
        "    \n",
        "    # Cache directories\n",
        "    \"PYTHONUNBUFFERED\": \"1\",\n",
        "    \"TRANSFORMERS_CACHE\": \"/workspace/cache/transformers\",\n",
        "    \"HF_HOME\": \"/workspace/cache\",\n",
        "    \"HF_DATASETS_CACHE\": \"/workspace/cache/datasets\",\n",
        "    \n",
        "    # Distributed training debug\n",
        "    \"NCCL_DEBUG\": \"INFO\",\n",
        "    \"NCCL_DEBUG_SUBSYS\": \"ALL\",\n",
        "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
        "    \"NCCL_IB_DISABLE\": \"1\",\n",
        "    \"NCCL_P2P_DISABLE\": \"1\",\n",
        "    \"NCCL_TREE_THRESHOLD\": \"0\",\n",
        "    \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
        "    \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
        "}\n",
        "\n",
        "# Create CustomTrainer configuration\n",
        "custom_trainer = CustomTrainer(\n",
        "    func=train_gpt2_with_trl_checkpointing,\n",
        "    func_args=training_args,\n",
        "    num_nodes=2,  # Distributed training across 2 nodes (matching YAML)\n",
        "    resources_per_node={\n",
        "        \"cpu\": \"2\",\n",
        "        \"memory\": \"4Gi\",\n",
        "        # Uncomment for GPU training:\n",
        "        # \"nvidia.com/gpu\": \"1\",\n",
        "    },\n",
        "    packages_to_install=[\n",
        "        \"transformers[torch]\",\n",
        "        \"trl\", \n",
        "        \"peft\", \n",
        "        \"datasets\", \n",
        "        \"accelerate\",\n",
        "        \"torch\",\n",
        "        \"numpy\"\n",
        "    ],\n",
        "    env={\n",
        "        \"PYTHONUNBUFFERED\": \"1\",\n",
        "        \"NCCL_DEBUG\": \"INFO\",\n",
        "        \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ CustomTrainer configured with TRL training function\")\n",
        "print(f\"üéØ Distributed training: {custom_trainer.num_nodes} nodes\")\n",
        "print(f\"üîß Environment variables: {len(training_args)} configured\")\n",
        "print(f\"üì¶ Packages to install: {len(custom_trainer.packages_to_install)} packages\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Configure Initializers (matching trl-trainjob.yaml V2 initializers)\n",
        "initializer = Initializer(\n",
        "    dataset=HuggingFaceDatasetInitializer(\n",
        "        storage_uri=\"hf://tatsu-lab/alpaca\"\n",
        "    ),\n",
        "    model=HuggingFaceModelInitializer(\n",
        "        storage_uri=\"hf://gpt2\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Initializer configured with V2 dataset and model initializers\")\n",
        "print(f\"üìä Dataset: {initializer.dataset.storage_uri}\")\n",
        "print(f\"ü§ñ Model: {initializer.model.storage_uri}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Create TrainJob using Kubeflow SDK (matching trl-trainjob.yaml procedure)\n",
        "job_name = trainer_client.train(\n",
        "    trainer=custom_trainer,\n",
        "    initializer=initializer,\n",
        "    labels={\n",
        "        \"app.kubernetes.io/name\": \"trl-demo\",\n",
        "        \"app.kubernetes.io/component\": \"training\",\n",
        "        \"experiment\": \"advanced-controller-checkpointing\"\n",
        "    },\n",
        "    annotations={\n",
        "        \"training.kubeflow.org/description\": \"TRL GPT-2 fine-tuning with advanced checkpointing using Kubeflow SDK\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ TrainJob created successfully using Kubeflow SDK!\")\n",
        "print(f\"üìã Job Name: {job_name}\")\n",
        "print(f\"üéØ This TrainJob matches the same procedure as trl-trainjob.yaml:\")\n",
        "print(f\"   - V2 Initializers for dataset and model\")\n",
        "print(f\"   - Distributed training across 2 nodes\")\n",
        "print(f\"   - Advanced TRL training with checkpointing\")\n",
        "print(f\"   - Controller-managed progress tracking\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìä Monitor Training Progress Using Kubeflow SDK\n",
        "\n",
        "Monitor the TrainJob progress using the Kubeflow SDK - same functionality as `kubectl get trainjob`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def monitor_training_with_sdk(duration_minutes=10, interval_seconds=30):\n",
        "    \"\"\"Monitor training progress using Kubeflow SDK\"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "    \n",
        "    end_time = datetime.now() + timedelta(minutes=duration_minutes)\n",
        "    \n",
        "    print(f\"üîç Monitoring TrainJob '{job_name}' for {duration_minutes} minutes\")\n",
        "    print(f\"üîÑ Checking every {interval_seconds} seconds using Kubeflow SDK\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    while datetime.now() < end_time:\n",
        "        print(f\"\\n‚è∞ {datetime.now().strftime('%H:%M:%S')} - Checking status...\")\n",
        "        \n",
        "        try:\n",
        "            # Get TrainJob status using SDK\n",
        "            trainjob = trainer_client.get_job(job_name)\n",
        "            \n",
        "            print(f\"üéØ TrainJob Status:\")\n",
        "            print(f\"   Name: {trainjob.name}\")\n",
        "            print(f\"   Status: {trainjob.status}\")\n",
        "            print(f\"   Nodes: {trainjob.num_nodes}\")\n",
        "            print(f\"   Runtime: {trainjob.runtime.name}\")\n",
        "            \n",
        "            if trainjob.steps:\n",
        "                print(f\"\\nüìã Steps:\")\n",
        "                for step in trainjob.steps:\n",
        "                    print(f\"   - {step.name}: {step.status} ({step.device})\")\n",
        "            \n",
        "            # Check if training is complete\n",
        "            if trainjob.status in ['Complete', 'Failed']:\n",
        "                print(f\"\\nüèÅ Training finished with status: {trainjob.status}\")\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error getting job status: {e}\")\n",
        "        \n",
        "        print(f\"\\n‚è≥ Waiting {interval_seconds} seconds...\")\n",
        "        time.sleep(interval_seconds)\n",
        "    \n",
        "    print(\"\\n‚úÖ Monitoring completed\")\n",
        "\n",
        "# Start monitoring\n",
        "monitor_training_with_sdk(duration_minutes=15, interval_seconds=30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìú View Training Logs Using Kubeflow SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get training logs using Kubeflow SDK\n",
        "try:\n",
        "    print(f\"üìú Getting logs for TrainJob: {job_name}\")\n",
        "    \n",
        "    # Get logs from the training nodes\n",
        "    logs = trainer_client.get_job_logs(job_name, follow=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING LOGS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Display logs from all nodes\n",
        "    for node_name, node_logs in logs.items():\n",
        "        print(f\"\\n--- {node_name.upper()} LOGS ---\")\n",
        "        # Display last 50 lines of logs\n",
        "        log_lines = node_logs.split('\\n')\n",
        "        for line in log_lines[-50:]:\n",
        "            if line.strip():\n",
        "                print(line)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error getting logs: {e}\")\n",
        "    print(\"Note: Logs may not be available yet if training is still starting up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Cleanup Using Kubeflow SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up the TrainJob when done\n",
        "def cleanup_trainjob():\n",
        "    \"\"\"Clean up the TrainJob using Kubeflow SDK\"\"\"\n",
        "    try:\n",
        "        trainer_client.delete_job(job_name)\n",
        "        print(f\"‚úÖ TrainJob '{job_name}' deleted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error deleting TrainJob: {e}\")\n",
        "\n",
        "# Get final job status before cleanup\n",
        "try:\n",
        "    final_job = trainer_client.get_job(job_name)\n",
        "    print(f\"üìã Final TrainJob Status:\")\n",
        "    print(f\"   Name: {final_job.name}\")\n",
        "    print(f\"   Status: {final_job.status}\")\n",
        "    print(f\"   Created: {final_job.creation_timestamp}\")\n",
        "    print(f\"   Nodes: {final_job.num_nodes}\")\n",
        "    print(f\"   Runtime: {final_job.runtime.name}\")\n",
        "    \n",
        "    if final_job.steps:\n",
        "        print(f\"   Steps:\")\n",
        "        for step in final_job.steps:\n",
        "            print(f\"     - {step.name}: {step.status}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error getting final job status: {e}\")\n",
        "\n",
        "print(f\"\\nüí° To delete the TrainJob, run: cleanup_trainjob()\")\n",
        "print(f\"Current TrainJob ID: {job_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Summary\n",
        "\n",
        "This notebook demonstrates how to use the **Kubeflow SDK** to create TrainJobs that match the same procedure as `trl-trainjob.yaml`:\n",
        "\n",
        "### üéØ **Key Features Implemented**:\n",
        "\n",
        "1. **Kubeflow SDK Integration**: Used `TrainerClient` with `CustomTrainer` instead of direct Kubernetes API calls\n",
        "2. **TRL-based fine-tuning** with GPT-2 and Alpaca dataset for instruction following\n",
        "3. **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning  \n",
        "4. **Advanced checkpointing** with SIGTERM handling and distributed coordination\n",
        "5. **Distributed training** across multiple nodes with automatic environment setup\n",
        "6. **V2 Initializers** for reproducible dataset and model preparation using `HuggingFaceDatasetInitializer` and `HuggingFaceModelInitializer`\n",
        "7. **Real-time monitoring** using SDK methods instead of Kubernetes API\n",
        "\n",
        "### üîß **Technical Implementation**:\n",
        "\n",
        "- **CustomTrainer**: Encapsulates the TRL training function with all dependencies\n",
        "- **Initializer**: Configures V2 dataset and model initializers (matching YAML)\n",
        "- **SDK Methods**: Uses `trainer_client.train()`, `get_job()`, `get_job_logs()`, `delete_job()`\n",
        "- **Distributed coordination**: SIGTERM tensor for cross-node communication\n",
        "- **Automatic checkpoint resumption**: Detection and loading of latest checkpoints\n",
        "\n",
        "### üìä **Advantages of SDK Approach**:\n",
        "\n",
        "- **Simplified API**: No need to manage Kubernetes manifests directly\n",
        "- **Type Safety**: Python types and validation for all parameters\n",
        "- **Error Handling**: Built-in retry logic and error messages\n",
        "- **Monitoring**: Native support for job status and log retrieval\n",
        "- **Portability**: Works across different Kubernetes clusters and configurations\n",
        "\n",
        "### üöÄ **Usage Pattern**:\n",
        "\n",
        "```python\n",
        "# 1. Configure training function and parameters\n",
        "custom_trainer = CustomTrainer(\n",
        "    func=train_gpt2_with_trl_checkpointing,\n",
        "    func_args=training_args,\n",
        "    num_nodes=2,\n",
        "    resources_per_node={\"cpu\": \"2\", \"memory\": \"4Gi\"},\n",
        "    packages_to_install=[\"transformers[torch]\", \"trl\", \"peft\", ...]\n",
        ")\n",
        "\n",
        "# 2. Configure initializers (matching YAML V2 initializers)\n",
        "initializer = Initializer(\n",
        "    dataset=HuggingFaceDatasetInitializer(\"hf://tatsu-lab/alpaca\"),\n",
        "    model=HuggingFaceModelInitializer(\"hf://gpt2\")\n",
        ")\n",
        "\n",
        "# 3. Create TrainJob\n",
        "job_name = trainer_client.train(\n",
        "    trainer=custom_trainer,\n",
        "    initializer=initializer,\n",
        "    labels={\"experiment\": \"trl-gpt2-checkpointing\"}\n",
        ")\n",
        "\n",
        "# 4. Monitor and manage\n",
        "trainjob = trainer_client.get_job(job_name)\n",
        "logs = trainer_client.get_job_logs(job_name)\n",
        "trainer_client.delete_job(job_name)\n",
        "```\n",
        "\n",
        "This approach provides the **same functionality as trl-trainjob.yaml** but with a more **Pythonic and maintainable interface** through the Kubeflow SDK.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_trainjob_status():\n",
        "    \"\"\"Get TrainJob status and training progress\"\"\"\n",
        "    try:\n",
        "        trainjob = custom_objects_api.get_namespaced_custom_object(\n",
        "            group=\"trainer.kubeflow.org\",\n",
        "            version=\"v1alpha1\",\n",
        "            namespace=NAMESPACE,\n",
        "            plural=\"trainjobs\",\n",
        "            name=\"trl-sdk-demo\"\n",
        "        )\n",
        "        \n",
        "        status = trainjob.get(\"status\", {})\n",
        "        conditions = status.get(\"conditions\", [])\n",
        "        training_progress = status.get(\"trainingProgress\", {})\n",
        "        checkpointing = status.get(\"checkpointing\", {})\n",
        "        \n",
        "        print(f\"üéØ TrainJob Status:\")\n",
        "        print(f\"   Phase: {status.get('phase', 'Unknown')}\")\n",
        "        \n",
        "        if conditions:\n",
        "            latest_condition = conditions[-1]\n",
        "            print(f\"   Condition: {latest_condition.get('type')} - {latest_condition.get('status')}\")\n",
        "            if latest_condition.get('message'):\n",
        "                print(f\"   Message: {latest_condition.get('message')}\")\n",
        "        \n",
        "        if training_progress:\n",
        "            print(f\"\\nüìà Training Progress:\")\n",
        "            print(f\"   Epoch: {training_progress.get('epoch', 0)}/{training_progress.get('totalEpochs', 0)}\")\n",
        "            print(f\"   Step: {training_progress.get('step', 0)}/{training_progress.get('totalSteps', 0)}\")\n",
        "            print(f\"   Loss: {training_progress.get('loss', 'N/A')}\")\n",
        "            print(f\"   Learning Rate: {training_progress.get('learningRate', 'N/A')}\")\n",
        "            print(f\"   Progress: {training_progress.get('percentComplete', '0')}%\")\n",
        "            print(f\"   Last Update: {training_progress.get('lastUpdateTime', 'N/A')}\")\n",
        "        \n",
        "        if checkpointing:\n",
        "            print(f\"\\nüíæ Checkpointing:\")\n",
        "            print(f\"   Enabled: {checkpointing.get('enabled', False)}\")\n",
        "            print(f\"   Checkpoints Created: {checkpointing.get('checkpointsCreated', 0)}\")\n",
        "            print(f\"   Latest Checkpoint: {checkpointing.get('latestCheckpointTime', 'N/A')}\")\n",
        "        \n",
        "        return trainjob\n",
        "        \n",
        "    except ApiException as e:\n",
        "        print(f\"‚ùå Failed to get TrainJob status: {e}\")\n",
        "        return None\n",
        "\n",
        "def monitor_training(duration_minutes=10, interval_seconds=30):\n",
        "    \"\"\"Monitor training progress for specified duration\"\"\"\n",
        "    import time\n",
        "    from datetime import datetime, timedelta\n",
        "    \n",
        "    end_time = datetime.now() + timedelta(minutes=duration_minutes)\n",
        "    \n",
        "    print(f\"üîç Monitoring training for {duration_minutes} minutes (checking every {interval_seconds}s)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    while datetime.now() < end_time:\n",
        "        print(f\"\\n‚è∞ {datetime.now().strftime('%H:%M:%S')} - Checking status...\")\n",
        "        trainjob = get_trainjob_status()\n",
        "        \n",
        "        if trainjob:\n",
        "            status = trainjob.get(\"status\", {})\n",
        "            phase = status.get(\"phase\", \"Unknown\")\n",
        "            \n",
        "            if phase in [\"Succeeded\", \"Failed\"]:\n",
        "                print(f\"\\nüèÅ Training completed with status: {phase}\")\n",
        "                break\n",
        "        \n",
        "        print(f\"\\n‚è≥ Waiting {interval_seconds} seconds...\")\n",
        "        time.sleep(interval_seconds)\n",
        "    \n",
        "    print(\"\\n‚úÖ Monitoring completed\")\n",
        "\n",
        "# Start monitoring\n",
        "monitor_training(duration_minutes=15, interval_seconds=30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Cleanup Resources\n",
        "\n",
        "Clean up the created resources when done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_resources():\n",
        "    \"\"\"Clean up all created resources\"\"\"\n",
        "    print(\"üßπ Cleaning up resources...\")\n",
        "    \n",
        "    # Delete TrainJob\n",
        "    try:\n",
        "        custom_objects_api.delete_namespaced_custom_object(\n",
        "            group=\"trainer.kubeflow.org\",\n",
        "            version=\"v1alpha1\",\n",
        "            namespace=NAMESPACE,\n",
        "            plural=\"trainjobs\",\n",
        "            name=\"trl-sdk-demo\"\n",
        "        )\n",
        "        print(\"‚úÖ TrainJob deleted\")\n",
        "    except ApiException as e:\n",
        "        if e.status == 404:\n",
        "            print(\"‚ö†Ô∏è  TrainJob not found\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to delete TrainJob: {e}\")\n",
        "    \n",
        "    # Delete TrainingRuntime\n",
        "    try:\n",
        "        custom_objects_api.delete_namespaced_custom_object(\n",
        "            group=\"trainer.kubeflow.org\",\n",
        "            version=\"v1alpha1\",\n",
        "            namespace=NAMESPACE,\n",
        "            plural=\"trainingruntimes\",\n",
        "            name=\"torch-cuda-251-runtime\"\n",
        "        )\n",
        "        print(\"‚úÖ TrainingRuntime deleted\")\n",
        "    except ApiException as e:\n",
        "        if e.status == 404:\n",
        "            print(\"‚ö†Ô∏è  TrainingRuntime not found\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to delete TrainingRuntime: {e}\")\n",
        "    \n",
        "    # Delete ConfigMap\n",
        "    try:\n",
        "        core_v1.delete_namespaced_config_map(\n",
        "            name=\"advanced-trl-script\",\n",
        "            namespace=NAMESPACE\n",
        "        )\n",
        "        print(\"‚úÖ ConfigMap deleted\")\n",
        "    except ApiException as e:\n",
        "        if e.status == 404:\n",
        "            print(\"‚ö†Ô∏è  ConfigMap not found\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to delete ConfigMap: {e}\")\n",
        "    \n",
        "    # Delete PVC (optional - comment out to preserve data)\n",
        "    # try:\n",
        "    #     core_v1.delete_namespaced_persistent_volume_claim(\n",
        "    #         name=\"shared-checkpoint-storage\",\n",
        "    #         namespace=NAMESPACE\n",
        "    #     )\n",
        "    #     print(\"‚úÖ PVC deleted\")\n",
        "    # except ApiException as e:\n",
        "    #     if e.status == 404:\n",
        "    #         print(\"‚ö†Ô∏è  PVC not found\")\n",
        "    #     else:\n",
        "    #         print(f\"‚ùå Failed to delete PVC: {e}\")\n",
        "    \n",
        "    print(\"üèÅ Cleanup completed!\")\n",
        "\n",
        "# Uncomment to run cleanup\n",
        "# cleanup_resources()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Create and Submit TrainJob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the TrainJob using Kubeflow SDK\n",
        "print(f\"üöÄ Creating TrainJob: {job_name}\")\n",
        "\n",
        "try:\n",
        "    trainer_client.train(\n",
        "        name=job_name,\n",
        "        trainer=training_config,\n",
        "        # Enable checkpointing (this will be handled by the enhanced controller)\n",
        "        # The controller will inject CHECKPOINT_ENABLED, TRAINING_PROGRESS_FILE, etc.\n",
        "    )\n",
        "    print(f\"‚úÖ TrainJob '{job_name}' created successfully!\")\n",
        "    print(f\"üìä Enhanced checkpointing and progress tracking enabled\")\n",
        "    print(f\"üîÑ Controller will poll progress every 15-60 seconds adaptively\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create TrainJob: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Monitor Training Progress\n",
        "\n",
        "Use the enhanced progress tracking capabilities to monitor training in real-time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor the TrainJob status and progress\n",
        "def monitor_training_progress(job_name, max_iterations=60, sleep_interval=10):\n",
        "    \"\"\"Monitor training progress using the enhanced checkpointing API\"\"\"\n",
        "    print(f\"üìä Monitoring TrainJob: {job_name}\")\n",
        "    print(f\"üîÑ Checking every {sleep_interval} seconds for up to {max_iterations} iterations\")\n",
        "    print(\"\" + \"=\"*80)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        try:\n",
        "            # Get TrainJob status\n",
        "            job = trainer_client.get_job(job_name)\n",
        "            \n",
        "            print(f\"\\\\n[{datetime.now().strftime('%H:%M:%S')}] Iteration {i+1}/{max_iterations}\")\n",
        "            print(f\"üìã Status: {job.status}\")\n",
        "            \n",
        "            # Check for training progress (enhanced checkpointing feature)\n",
        "            if hasattr(job, 'training_progress') and job.training_progress:\n",
        "                progress = job.training_progress\n",
        "                print(f\"üìà Progress Details:\")\n",
        "                if hasattr(progress, 'epoch'):\n",
        "                    print(f\"   üìÖ Epoch: {progress.epoch}/{getattr(progress, 'total_epochs', '?')}\")\n",
        "                if hasattr(progress, 'step'):\n",
        "                    print(f\"   üë£ Step: {progress.step}/{getattr(progress, 'total_steps', '?')}\")\n",
        "                if hasattr(progress, 'loss'):\n",
        "                    print(f\"   üìâ Loss: {progress.loss}\")\n",
        "                if hasattr(progress, 'learning_rate'):\n",
        "                    print(f\"   üéØ Learning Rate: {progress.learning_rate}\")\n",
        "                if hasattr(progress, 'percent_complete'):\n",
        "                    print(f\"   üìä Progress: {progress.percent_complete}%\")\n",
        "                if hasattr(progress, 'last_update_time'):\n",
        "                    print(f\"   üïí Last Update: {progress.last_update_time}\")\n",
        "                \n",
        "                # Check checkpointing status\n",
        "                if hasattr(progress, 'checkpointing') and progress.checkpointing:\n",
        "                    checkpoint_status = progress.checkpointing\n",
        "                    print(f\"üíæ Checkpointing:\")\n",
        "                    if hasattr(checkpoint_status, 'enabled'):\n",
        "                        print(f\"   ‚úÖ Enabled: {checkpoint_status.enabled}\")\n",
        "                    if hasattr(checkpoint_status, 'latest_checkpoint'):\n",
        "                        print(f\"   üìÅ Latest: {checkpoint_status.latest_checkpoint}\")\n",
        "                    if hasattr(checkpoint_status, 'checkpoints_created'):\n",
        "                        print(f\"   üìä Created: {checkpoint_status.checkpoints_created}\")\n",
        "            else:\n",
        "                print(\"üìä No progress data available yet (controller may still be initializing)\")\n",
        "            \n",
        "            # Check if training is complete\n",
        "            if job.status in ['Succeeded', 'Failed', 'Complete']:\n",
        "                print(f\"\\\\nüéØ Training finished with status: {job.status}\")\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error getting job status: {e}\")\n",
        "        \n",
        "        if i < max_iterations - 1:  # Don't sleep on the last iteration\n",
        "            time.sleep(sleep_interval)\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"üìä Monitoring complete\")\n",
        "\n",
        "# Start monitoring\n",
        "monitor_training_progress(job_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Get Detailed Job Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get comprehensive job information\n",
        "try:\n",
        "    job = trainer_client.get_job(job_name)\n",
        "    \n",
        "    print(f\"üìã TrainJob Details: {job_name}\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Status: {job.status}\")\n",
        "    print(f\"Created: {job.creation_timestamp}\")\n",
        "    \n",
        "    if hasattr(job, 'start_time') and job.start_time:\n",
        "        print(f\"Started: {job.start_time}\")\n",
        "    \n",
        "    if hasattr(job, 'completion_time') and job.completion_time:\n",
        "        print(f\"Completed: {job.completion_time}\")\n",
        "    \n",
        "    # Enhanced progress information\n",
        "    if hasattr(job, 'training_progress') and job.training_progress:\n",
        "        print(\"\\\\nüìä Enhanced Progress Tracking:\")\n",
        "        progress = job.training_progress\n",
        "        \n",
        "        for attr in ['epoch', 'total_epochs', 'step', 'total_steps', 'loss', \n",
        "                     'accuracy', 'learning_rate', 'percent_complete', 'last_update_time']:\n",
        "            if hasattr(progress, attr):\n",
        "                value = getattr(progress, attr)\n",
        "                if value is not None:\n",
        "                    print(f\"  {attr.replace('_', ' ').title()}: {value}\")\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error getting job details: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìú View Training Logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get training logs\n",
        "try:\n",
        "    print(f\"üìú Getting logs for TrainJob: {job_name}\")\n",
        "    logs = trainer_client.get_job_logs(job_name, follow=False)\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING LOGS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Display last 50 lines of logs\n",
        "    log_lines = logs.split('\\\\n')\n",
        "    for line in log_lines[-50:]:\n",
        "        if line.strip():\n",
        "            print(line)\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error getting logs: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Cleanup (Optional)\n",
        "\n",
        "Clean up the TrainJob when you're done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_gpt2_with_checkpointing(args):\n",
        "    import random\n",
        "    import os\n",
        "    import torch\n",
        "    import numpy\n",
        "    from numpy.core.multiarray import _reconstruct\n",
        "    import torch.serialization\n",
        "    torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])\n",
        "    from datetime import datetime\n",
        "    import signal\n",
        "    import torch.distributed as dist\n",
        "    import logging\n",
        "    from pathlib import Path\n",
        "    from cloudpathlib import CloudPath\n",
        "    \n",
        "    from datasets import load_dataset\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        TrainingArguments,\n",
        "        TrainerState,\n",
        "        TrainerControl,\n",
        "        TrainerCallback,\n",
        "        set_seed,\n",
        "    )\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "    \n",
        "    from trl import (\n",
        "        ModelConfig,\n",
        "        ScriptArguments,\n",
        "        SFTConfig,\n",
        "        SFTTrainer,\n",
        "        TrlParser,\n",
        "        get_peft_config,\n",
        "    )\n",
        "\n",
        "    class SigtermCheckpointCallback(TrainerCallback):\n",
        "        \"\"\"Advanced checkpoint callback with SIGTERM handling and distributed coordination.\"\"\"\n",
        "        \n",
        "        def __init__(self, output_dir: str):\n",
        "            self.output_dir = output_dir\n",
        "            self.checkpoint_requested = False\n",
        "            self.save_triggered = False\n",
        "            self.checkpoint_stream = None\n",
        "            self.sigterm_tensor = None\n",
        "\n",
        "        def _log_message(self, message: str):\n",
        "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            rank = os.environ.get(\"RANK\", \"0\")\n",
        "            print(f\"[Rank {rank}] [{timestamp}] {message}\")\n",
        "\n",
        "        def _init_distributed_signal_tensor(self):\n",
        "            try:\n",
        "                if dist.is_initialized():\n",
        "                    device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')\n",
        "                    self.sigterm_tensor = torch.zeros(1, dtype=torch.float32, device=device)\n",
        "                    self._log_message(f\"Initialized distributed SIGTERM tensor on device: {device}\")\n",
        "                else:\n",
        "                    self._log_message(\"Distributed training not initialized - using local SIGTERM handling only\")\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Failed to initialize distributed SIGTERM tensor: {e}. Using local handling only.\")\n",
        "\n",
        "        def _check_distributed_sigterm(self):\n",
        "            try:\n",
        "                if dist.is_initialized() and self.sigterm_tensor is not None:\n",
        "                    dist.all_reduce(self.sigterm_tensor, op=dist.ReduceOp.MAX)\n",
        "                    return self.sigterm_tensor.item() > 0.5\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Distributed SIGTERM check failed: {e}. Using local signal only.\")\n",
        "            return self.checkpoint_requested\n",
        "\n",
        "        def _sigterm_handler(self, signum, frame):\n",
        "            self._log_message(f\"SIGTERM received, flagging for checkpoint.\")\n",
        "            self.checkpoint_requested = True\n",
        "            if self.sigterm_tensor is not None:\n",
        "                self.sigterm_tensor.fill_(1.0)\n",
        "\n",
        "        def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            rank = os.environ.get(\"RANK\", \"0\")\n",
        "            os.makedirs(self.output_dir, exist_ok=True)\n",
        "            self._init_distributed_signal_tensor()\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                self.checkpoint_stream = torch.cuda.Stream()\n",
        "                self._log_message(f\"Created dedicated CUDA stream for checkpointing.\")\n",
        "\n",
        "            signal.signal(signal.SIGTERM, self._sigterm_handler)\n",
        "            self._log_message(f\"SIGTERM signal handler registered for distributed coordination.\")\n",
        "\n",
        "            try:\n",
        "                if dist.is_initialized():\n",
        "                    dist.barrier()\n",
        "                    self._log_message(f\"Distributed coordination setup synchronized across all ranks\")\n",
        "            except Exception as e:\n",
        "                self._log_message(f\"Failed to synchronize distributed setup: {e}\")\n",
        "\n",
        "        def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            if self._check_distributed_sigterm() and not self.save_triggered:\n",
        "                rank = os.environ.get(\"RANK\", \"0\")\n",
        "                self._log_message(f\"Distributed SIGTERM detected, initiating checkpoint at step {state.global_step}.\")\n",
        "                self.save_triggered = True\n",
        "                control.should_save = True\n",
        "                control.should_training_stop = True\n",
        "\n",
        "        def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "            rank = os.environ.get(\"RANK\", \"0\")\n",
        "            if rank != \"0\":\n",
        "                return\n",
        "            self._log_message(f\"Checkpoint save completed.\")\n",
        "            if self.checkpoint_requested:\n",
        "                self._log_message(f\"Distributed SIGTERM-triggered checkpoint save finished successfully.\")\n",
        "\n",
        "    # Setup checkpoint directory\n",
        "    checkpoint_dir = Path(args.get('CHECKPOINT_DIR', '/tmp/checkpoints'))\n",
        "    \n",
        "    # TRL configuration parameters\n",
        "    parameters = {\n",
        "        'model_name_or_path': args.get('MODEL_NAME', 'gpt2'),\n",
        "        'model_revision': 'main',\n",
        "        'torch_dtype': 'bfloat16',\n",
        "        'use_peft': True,\n",
        "        'lora_r': int(args.get('LORA_R', '16')),\n",
        "        'lora_alpha': int(args.get('LORA_ALPHA', '8')),\n",
        "        'lora_dropout': float(args.get('LORA_DROPOUT', '0.05')),\n",
        "        'lora_target_modules': ['c_attn', 'c_proj'],\n",
        "        'dataset_name': args.get('DATASET_NAME', 'gsm8k'),\n",
        "        'dataset_config': 'main',\n",
        "        'dataset_train_split': args.get('TRAIN_SPLIT', 'train[:100]'),\n",
        "        'dataset_test_split': args.get('TEST_SPLIT', 'test[:20]'),\n",
        "        'max_seq_length': 512,\n",
        "        'num_train_epochs': int(args.get('MAX_EPOCHS', '1')),\n",
        "        'per_device_train_batch_size': int(args.get('BATCH_SIZE', '4')),\n",
        "        'per_device_eval_batch_size': int(args.get('BATCH_SIZE', '4')),\n",
        "        'eval_strategy': 'steps',\n",
        "        'eval_steps': int(args.get('EVAL_STEPS', '25')),\n",
        "        'bf16': True,\n",
        "        'learning_rate': float(args.get('LEARNING_RATE', '2e-4')),\n",
        "        'warmup_steps': int(args.get('WARMUP_STEPS', '10')),\n",
        "        'lr_scheduler_type': 'cosine',\n",
        "        'optim': 'adamw_torch',\n",
        "        'max_grad_norm': 1.0,\n",
        "        'seed': 42,\n",
        "        'gradient_accumulation_steps': 1,\n",
        "        'save_strategy': 'steps',\n",
        "        'save_steps': int(args.get('SAVE_STEPS', '50')),\n",
        "        'save_total_limit': 5,\n",
        "        'logging_strategy': 'steps',\n",
        "        'logging_steps': int(args.get('LOGGING_STEPS', '10')),\n",
        "        'report_to': [],\n",
        "        'output_dir': str(checkpoint_dir),\n",
        "        'dataloader_pin_memory': False,\n",
        "        'gradient_checkpointing': True,\n",
        "    }\n",
        "\n",
        "    # Create cache directories\n",
        "    cache_dirs = [\n",
        "        args.get('TRANSFORMERS_CACHE', '/tmp/transformers_cache'),\n",
        "        args.get('HF_HOME', '/tmp/hf_cache'),\n",
        "        args.get('HF_DATASETS_CACHE', '/tmp/hf_datasets_cache'),\n",
        "        str(checkpoint_dir)\n",
        "    ]\n",
        "    \n",
        "    for cache_dir in cache_dirs:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "    \n",
        "    rank = os.environ.get(\"RANK\", \"0\")\n",
        "    world_size = os.environ.get(\"WORLD_SIZE\", \"1\")\n",
        "    local_rank = os.environ.get(\"LOCAL_RANK\", \"0\")\n",
        "    \n",
        "    print(f\"[Rank {rank}] Starting TRL-based distributed training with advanced checkpointing\")\n",
        "    print(f\"[Rank {rank}] World Size: {world_size}, Local Rank: {local_rank}\")\n",
        "    print(f\"[Rank {rank}] Checkpoints will be saved to: {checkpoint_dir}\")\n",
        "    print(f\"[Rank {rank}] Model: {parameters['model_name_or_path']}\")\n",
        "    print(f\"[Rank {rank}] Dataset: {parameters['dataset_name']} ({parameters['dataset_train_split']})\")\n",
        "    print(f\"[Rank {rank}] GPU Available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"[Rank {rank}] GPU Count: {torch.cuda.device_count()}\")\n",
        "        print(f\"[Rank {rank}] Current GPU: {torch.cuda.current_device()}\")\n",
        "\n",
        "    # Parse TRL configuration\n",
        "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
        "    script_args, training_args, model_args = parser.parse_dict(parameters)\n",
        "\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    print(f\"[Rank {rank}] Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path, \n",
        "        trust_remote_code=model_args.trust_remote_code, \n",
        "        use_fast=True\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Add chat template for GPT-2\n",
        "    if tokenizer.chat_template is None:\n",
        "        tokenizer.chat_template = (\n",
        "            \"{% for message in messages %}\"\n",
        "            \"{% if message['role'] == 'user' %}\"\n",
        "            \"Question: {{ message['content'] }}\\\\n\"\n",
        "            \"{% elif message['role'] == 'assistant' %}\"\n",
        "            \"Answer: {{ message['content'] }}{{ eos_token }}\\\\n\"\n",
        "            \"{% endif %}\"\n",
        "            \"{% endfor %}\"\n",
        "        )\n",
        "\n",
        "    print(f\"[Rank {rank}] Loading dataset...\")\n",
        "    train_dataset = load_dataset(\n",
        "        path=script_args.dataset_name,\n",
        "        name=script_args.dataset_config,\n",
        "        split=script_args.dataset_train_split,\n",
        "    )\n",
        "    test_dataset = None\n",
        "    if training_args.eval_strategy != \"no\":\n",
        "        test_dataset = load_dataset(\n",
        "            path=script_args.dataset_name,\n",
        "            name=script_args.dataset_config,\n",
        "            split=script_args.dataset_test_split,\n",
        "        )\n",
        "\n",
        "    def template_dataset(sample):\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": sample['question']},\n",
        "            {\"role\": \"assistant\", \"content\": sample['answer']},\n",
        "        ]\n",
        "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "\n",
        "    print(f\"[Rank {rank}] Preprocessing datasets...\")\n",
        "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"question\", \"answer\"])\n",
        "    if test_dataset is not None:\n",
        "        test_dataset = test_dataset.map(template_dataset, remove_columns=[\"question\", \"answer\"])\n",
        "\n",
        "    print(f\"[Rank {rank}] Training samples: {len(train_dataset)}\")\n",
        "    if test_dataset:\n",
        "        print(f\"[Rank {rank}] Evaluation samples: {len(test_dataset)}\")\n",
        "\n",
        "    if rank == \"0\":\n",
        "        print(f\"[Rank {rank}] Sample training data:\")\n",
        "        for i in random.sample(range(len(train_dataset)), min(2, len(train_dataset))):\n",
        "            print(f\"Sample {i}: {train_dataset[i]['text'][:200]}...\")\n",
        "\n",
        "    print(f\"[Rank {rank}] Initializing SFTTrainer with checkpointing...\")\n",
        "    trainer = SFTTrainer(\n",
        "        model=model_args.model_name_or_path,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        peft_config=get_peft_config(model_args),\n",
        "        processing_class=tokenizer,\n",
        "        callbacks=[SigtermCheckpointCallback(str(checkpoint_dir))],\n",
        "    )\n",
        "\n",
        "    if trainer.accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
        "        trainer.model.print_trainable_parameters()\n",
        "\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "    if checkpoint is None:\n",
        "        print(f\"[Rank {rank}] No checkpoint found, starting training from scratch.\")\n",
        "    else:\n",
        "        print(f\"[Rank {rank}] Resuming from checkpoint: {checkpoint}\")\n",
        "\n",
        "    print(f\"[Rank {rank}] Starting training...\")\n",
        "    trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_model(training_args.output_dir)\n",
        "    print(f\"[Rank {rank}] Training completed, model checkpoint written to {training_args.output_dir}\")\n",
        "    \n",
        "    # Upload to cloud storage if specified\n",
        "    if args.get(\"BUCKET\", None):\n",
        "        print(f\"[Rank {rank}] Uploading model to {args['BUCKET']}\")\n",
        "        (CloudPath(args[\"BUCKET\"]) / \"gpt2-gsm8k-checkpoints\").upload_from(str(checkpoint_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List available training runtimes\n",
        "\n",
        "Check what training runtimes are available in your cluster:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
        "\n",
        "print(\"Available Training Runtimes:\")\n",
        "for r in TrainerClient().list_runtimes():\n",
        "    print(f\"Name: {r.name}, Framework: {r.trainer.framework.value}, Trainer Type: {r.trainer.trainer_type.value}\")\n",
        "    print(f\"Entrypoint: {r.trainer.entrypoint[:3]}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure training parameters\n",
        "\n",
        "Set up the training configuration with checkpointing and cloud storage options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To upload checkpoints to object storage (S3, GCS or Azure Blob Storage), \n",
        "# set the bucket with protocol, e.g., \"s3://my-bucket/checkpoints\"\n",
        "BUCKET = None\n",
        "\n",
        "# Training configuration\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DATASET_NAME = \"gsm8k\"\n",
        "CHECKPOINT_DIR = \"/tmp/checkpoints\"\n",
        "\n",
        "# Training hyperparameters\n",
        "args = {\n",
        "    \"BUCKET\": BUCKET,\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"DATASET_NAME\": DATASET_NAME,\n",
        "    \"CHECKPOINT_DIR\": CHECKPOINT_DIR,\n",
        "    \"TRAIN_SPLIT\": \"train[:100]\",\n",
        "    \"TEST_SPLIT\": \"test[:20]\",\n",
        "    \"LEARNING_RATE\": \"2e-4\",\n",
        "    \"BATCH_SIZE\": \"4\",\n",
        "    \"MAX_EPOCHS\": \"1\",\n",
        "    \"WARMUP_STEPS\": \"10\",\n",
        "    \"EVAL_STEPS\": \"25\",\n",
        "    \"SAVE_STEPS\": \"50\",\n",
        "    \"LOGGING_STEPS\": \"10\",\n",
        "    \"LORA_R\": \"16\",\n",
        "    \"LORA_ALPHA\": \"8\",\n",
        "    \"LORA_DROPOUT\": \"0.05\",\n",
        "    \"TRANSFORMERS_CACHE\": \"/tmp/transformers_cache\",\n",
        "    \"HF_HOME\": \"/tmp/hf_cache\",\n",
        "    \"HF_DATASETS_CACHE\": \"/tmp/hf_datasets_cache\",\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for key, value in args.items():\n",
        "    if value is not None:\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and submit the TrainJob\n",
        "\n",
        "Submit the training job with distributed training and checkpointing enabled:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_id = TrainerClient().train(\n",
        "    trainer=CustomTrainer(\n",
        "        func=train_gpt2_with_checkpointing,\n",
        "        func_args=args,\n",
        "        num_nodes=2,  # Distributed training across 2 nodes\n",
        "        packages_to_install=[\n",
        "            \"datasets\", \n",
        "            \"transformers[torch]\", \n",
        "            \"trl\", \n",
        "            \"peft\", \n",
        "            \"accelerate\",\n",
        "            \"cloudpathlib[all]\"\n",
        "        ],\n",
        "        resources_per_node={\n",
        "            \"cpu\": \"4\",\n",
        "            \"memory\": \"8Gi\",\n",
        "            # Uncomment this to use GPU nodes for training\n",
        "            # \"nvidia.com/gpu\": 1,\n",
        "        },\n",
        "        # Environment variables for distributed training debugging\n",
        "        env={\n",
        "            \"NCCL_DEBUG\": \"INFO\",\n",
        "            \"NCCL_DEBUG_SUBSYS\": \"ALL\",\n",
        "            \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
        "            \"PYTHONUNBUFFERED\": \"1\",\n",
        "        }\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"TrainJob submitted with ID: {job_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor the TrainJob\n",
        "\n",
        "Check the status and details of the submitted TrainJob:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the TrainJob details\n",
        "print(\"Recent TrainJobs:\")\n",
        "for job in TrainerClient().list_jobs():\n",
        "    print(f\"TrainJob: {job.name}, Status: {job.status}, Created at: {job.creation_timestamp}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for the job to start running\n",
        "import time\n",
        "\n",
        "def wait_for_job_running(job_id, max_wait=300):\n",
        "    \"\"\"Wait for the TrainJob to reach running status.\"\"\"\n",
        "    for i in range(max_wait // 5):\n",
        "        try:\n",
        "            trainjob = TrainerClient().get_job(name=job_id)\n",
        "            for step in trainjob.steps:\n",
        "                if step.status == \"Running\":\n",
        "                    print(f\"TrainJob {job_id} is now running!\")\n",
        "                    return True\n",
        "            print(f\"Waiting for TrainJob to start running... ({i*5}s/{max_wait}s)\")\n",
        "            time.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"Error checking job status: {e}\")\n",
        "            time.sleep(5)\n",
        "    \n",
        "    print(f\"TrainJob did not start running within {max_wait} seconds\")\n",
        "    return False\n",
        "\n",
        "wait_for_job_running(job_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show detailed job information\n",
        "try:\n",
        "    trainjob = TrainerClient().get_job(name=job_id)\n",
        "    print(f\"TrainJob Details for {job_id}:\")\n",
        "    print(f\"  Status: {trainjob.status}\")\n",
        "    print(f\"  Steps:\")\n",
        "    for step in trainjob.steps:\n",
        "        print(f\"    - Step: {step.name}, Status: {step.status}, Devices: {step.device} x {step.device_count}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting job details: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View training logs\n",
        "\n",
        "Monitor the training progress and checkpointing in real-time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the TrainJob logs with checkpointing information\n",
        "try:\n",
        "    print(f\"Streaming logs for TrainJob {job_id}...\")\n",
        "    print(\"Look for:\")\n",
        "    print(\"  - SIGTERM checkpoint callback initialization\")\n",
        "    print(\"  - Distributed training coordination\")\n",
        "    print(\"  - LoRA parameter information\")\n",
        "    print(\"  - Checkpoint saves every 50 steps\")\n",
        "    print(\"  - Evaluation every 25 steps\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    _ = TrainerClient().get_job_logs(name=job_id, follow=True)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\\\nLog streaming interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"Error streaming logs: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wait for training completion\n",
        "\n",
        "Use the SDK's wait functionality to monitor training completion:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for the TrainJob to complete (optional)\n",
        "# Uncomment to wait for completion\n",
        "# try:\n",
        "#     print(f\"Waiting for TrainJob {job_id} to complete...\")\n",
        "#     completed_job = TrainerClient().wait_for_job_status(\n",
        "#         name=job_id,\n",
        "#         status={\"Complete\"},\n",
        "#         timeout=1800,  # 30 minutes\n",
        "#         polling_interval=30\n",
        "#     )\n",
        "#     print(f\"TrainJob {job_id} completed successfully!\")\n",
        "#     print(f\"Final status: {completed_job.status}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error waiting for job completion: {e}\")\n",
        "\n",
        "print(\"Training job monitoring setup complete. Uncomment above to wait for completion.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test checkpoint resumption with Initializers\n",
        "\n",
        "Demonstrate how to use Kubeflow SDK with initializers for reproducible training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of using initializers with the Kubeflow SDK\n",
        "from kubeflow.trainer import Initializer, HuggingFaceDatasetInitializer, HuggingFaceModelInitializer\n",
        "\n",
        "# Create initializer configuration\n",
        "initializer = Initializer(\n",
        "    dataset=HuggingFaceDatasetInitializer(\n",
        "        storage_uri=\"hf://openai/gsm8k\"\n",
        "    ),\n",
        "    model=HuggingFaceModelInitializer(\n",
        "        storage_uri=\"hf://gpt2\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Example of resuming training with initializers (uncomment to test)\n",
        "# resume_args = args.copy()\n",
        "# resume_args[\"MAX_EPOCHS\"] = \"2\"  # Train for more epochs\n",
        "# \n",
        "# resume_job_id = TrainerClient().train(\n",
        "#     trainer=CustomTrainer(\n",
        "#         func=train_gpt2_with_checkpointing,\n",
        "#         func_args=resume_args,\n",
        "#         num_nodes=2,\n",
        "#         packages_to_install=[\n",
        "#             \"datasets\", \"transformers[torch]\", \"trl\", \"peft\", \"accelerate\", \"cloudpathlib[all]\"\n",
        "#         ],\n",
        "#         resources_per_node={\n",
        "#             \"cpu\": \"4\",\n",
        "#             \"memory\": \"8Gi\",\n",
        "#         },\n",
        "#         env={\n",
        "#             \"NCCL_DEBUG\": \"INFO\",\n",
        "#             \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
        "#             \"PYTHONUNBUFFERED\": \"1\",\n",
        "#         }\n",
        "#     ),\n",
        "#     initializer=initializer,  # Use initializers for reproducible training\n",
        "#     labels={\n",
        "#         \"experiment\": \"gpt2-gsm8k-resume\",\n",
        "#         \"checkpoint-strategy\": \"resume\"\n",
        "#     },\n",
        "#     annotations={\n",
        "#         \"training.kubeflow.org/description\": \"Resume training from checkpoint with initializers\"\n",
        "#     }\n",
        "# )\n",
        "# \n",
        "# print(f\"Resume TrainJob submitted with ID: {resume_job_id}\")\n",
        "\n",
        "print(\"Initializer configuration ready. Uncomment to test checkpoint resumption with initializers.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint management utilities\n",
        "\n",
        "Utility functions for managing and analyzing checkpoints:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_checkpoints(checkpoint_dir):\n",
        "    \"\"\"Analyze available checkpoints and their properties.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    \n",
        "    checkpoint_path = Path(checkpoint_dir)\n",
        "    \n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Analyzing checkpoints in {checkpoint_dir}:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Find all checkpoint directories\n",
        "    checkpoints = []\n",
        "    for item in checkpoint_path.iterdir():\n",
        "        if item.is_dir() and item.name.startswith('checkpoint-'):\n",
        "            checkpoints.append(item)\n",
        "    \n",
        "    if not checkpoints:\n",
        "        print(\"No checkpoints found\")\n",
        "        return\n",
        "    \n",
        "    # Sort checkpoints by step number\n",
        "    checkpoints.sort(key=lambda x: int(x.name.split('-')[1]))\n",
        "    \n",
        "    for checkpoint in checkpoints:\n",
        "        step = checkpoint.name.split('-')[1]\n",
        "        \n",
        "        # Get checkpoint size\n",
        "        total_size = sum(f.stat().st_size for f in checkpoint.rglob('*') if f.is_file())\n",
        "        size_mb = total_size / (1024 * 1024)\n",
        "        \n",
        "        # Check for trainer state\n",
        "        trainer_state_file = checkpoint / 'trainer_state.json'\n",
        "        training_info = \"N/A\"\n",
        "        \n",
        "        if trainer_state_file.exists():\n",
        "            try:\n",
        "                with open(trainer_state_file, 'r') as f:\n",
        "                    state = json.load(f)\n",
        "                    epoch = state.get('epoch', 'N/A')\n",
        "                    global_step = state.get('global_step', 'N/A')\n",
        "                    training_info = f\"Epoch: {epoch}, Step: {global_step}\"\n",
        "            except Exception as e:\n",
        "                training_info = f\"Error reading state: {e}\"\n",
        "        \n",
        "        print(f\"Checkpoint: {checkpoint.name}\")\n",
        "        print(f\"  Size: {size_mb:.2f} MB\")\n",
        "        print(f\"  Training Info: {training_info}\")\n",
        "        print(f\"  Files: {len(list(checkpoint.rglob('*')))}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "def test_fine_tuned_model(checkpoint_path):\n",
        "    \"\"\"Test the fine-tuned GPT-2 model on mathematical reasoning.\"\"\"\n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "        from peft import PeftModel\n",
        "        import torch\n",
        "        \n",
        "        # Load the base model and tokenizer\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        \n",
        "        # Load the LoRA adapter\n",
        "        model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
        "        \n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"Question: If John has 5 apples and gives 2 to Mary, how many apples does John have left?\\\\nAnswer:\",\n",
        "            \"Question: A rectangle has a length of 8 meters and a width of 3 meters. What is its area?\\\\nAnswer:\",\n",
        "            \"Question: If a train travels 60 miles in 2 hours, what is its average speed?\\\\nAnswer:\"\n",
        "        ]\n",
        "        \n",
        "        print(\"Testing fine-tuned GPT-2 model on mathematical reasoning:\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        for i, question in enumerate(test_questions, 1):\n",
        "            inputs = tokenizer(question, return_tensors=\"pt\")\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            print(f\"Test {i}:\")\n",
        "            print(f\"Input: {question}\")\n",
        "            print(f\"Output: {response[len(question):].strip()}\")\n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error testing model: {e}\")\n",
        "        print(\"Make sure training has completed and checkpoints are available.\")\n",
        "\n",
        "# Example usage\n",
        "# analyze_checkpoints(\"/tmp/checkpoints\")\n",
        "# test_fine_tuned_model(\"/path/to/checkpoint\")\n",
        "print(\"Checkpoint analysis and model testing utilities ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up\n",
        "\n",
        "Clean up resources when done with the training job:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete the TrainJob\n",
        "# try:\n",
        "#     TrainerClient().delete_job(job_id)\n",
        "#     print(f\"TrainJob {job_id} deleted successfully\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error deleting TrainJob: {e}\")\n",
        "\n",
        "print(\"To delete the TrainJob, uncomment the lines above\")\n",
        "print(f\"Current TrainJob ID: {job_id}\")\n",
        "\n",
        "# Show final job status\n",
        "try:\n",
        "    final_job = TrainerClient().get_job(name=job_id)\n",
        "    print(f\"\\\\nFinal TrainJob Status: {final_job.status}\")\n",
        "    print(\"Steps:\")\n",
        "    for step in final_job.steps:\n",
        "        print(f\"  - {step.name}: {step.status}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting final job status: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated advanced checkpointing features with TRL-based GPT-2 fine-tuning using the Kubeflow SDK:\n",
        "\n",
        "### üéØ **Key Features Implemented**:\n",
        "\n",
        "1. **TRL-based fine-tuning** with GPT-2 and GSM8K dataset for mathematical reasoning\n",
        "2. **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning  \n",
        "3. **Advanced checkpointing** with SIGTERM handling and distributed coordination\n",
        "4. **Distributed training** across multiple nodes with automatic environment setup\n",
        "5. **Kubeflow SDK integration** with proper error handling and monitoring\n",
        "6. **Initializers support** for reproducible dataset and model preparation\n",
        "7. **Checkpoint management** utilities for analysis and resumption\n",
        "\n",
        "### üîß **Technical Implementation**:\n",
        "\n",
        "- **SigtermCheckpointCallback**: Custom callback for graceful shutdown and distributed checkpointing\n",
        "- **Distributed coordination**: SIGTERM tensor for cross-node communication\n",
        "- **Automatic checkpoint resumption**: Detection and loading of latest checkpoints\n",
        "- **Cloud storage integration**: Optional upload to S3/GCS/Azure for backup\n",
        "- **Comprehensive monitoring**: Real-time logs and status tracking\n",
        "\n",
        "### üìä **Checkpointing Strategy**:\n",
        "\n",
        "- **Automatic saves** every 50 training steps\n",
        "- **Evaluation** every 25 steps with metrics tracking\n",
        "- **SIGTERM signal handling** for graceful shutdown on interruption\n",
        "- **Distributed coordination** ensures all nodes checkpoint consistently\n",
        "- **Resume capability** from latest checkpoint on restart\n",
        "\n",
        "### üöÄ **Production Features**:\n",
        "\n",
        "- **Fault tolerance** with automatic restart and resume\n",
        "- **Resource optimization** with gradient checkpointing and memory management\n",
        "- **Monitoring integration** with comprehensive logging and status tracking\n",
        "- **Scalability** with distributed training across multiple nodes\n",
        "- **Reproducibility** with initializers and deterministic training\n",
        "\n",
        "### üí° **Usage Patterns**:\n",
        "\n",
        "```python\n",
        "# Basic training with checkpointing\n",
        "job_id = TrainerClient().train(\n",
        "    trainer=CustomTrainer(func=train_gpt2_with_checkpointing, ...),\n",
        "    labels={\"experiment\": \"gpt2-gsm8k\"},\n",
        "    annotations={\"description\": \"TRL fine-tuning with checkpointing\"}\n",
        ")\n",
        "\n",
        "# With initializers for reproducibility\n",
        "job_id = TrainerClient().train(\n",
        "    trainer=CustomTrainer(...),\n",
        "    initializer=Initializer(\n",
        "        dataset=HuggingFaceDatasetInitializer(\"hf://openai/gsm8k\"),\n",
        "        model=HuggingFaceModelInitializer(\"hf://gpt2\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Monitor and wait for completion\n",
        "TrainerClient().wait_for_job_status(job_id, {\"Complete\"})\n",
        "logs = TrainerClient().get_job_logs(job_id, follow=True)\n",
        "```\n",
        "\n",
        "This approach ensures **robust, fault-tolerant training** that can handle interruptions and resume seamlessly, making it suitable for production ML workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
