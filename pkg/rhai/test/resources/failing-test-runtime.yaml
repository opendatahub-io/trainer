apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainingRuntime
metadata:
  name: failing-test-runtime
spec:
  mlPolicy:
    numNodes: 1
    torch:
      numProcPerNode: auto
  template:
    spec:
      replicatedJobs:
        - name: node
          template:
            metadata:
              labels:
                trainer.kubeflow.org/trainjob-ancestor-step: trainer
            spec:
              backoffLimit: 0
              template:
                spec:
                  containers:
                    - name: node
                      image: quay.io/modh/training:py311-cuda124-torch251
                      command:
                        - python
                        - "-c"
                        - |
                          # Replicate SDK's progression tracking - test failure scenario
                          # Callback should still write termination message even when training fails
                          import os
                          os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

                          import json
                          import time
                          import threading
                          import sys
                          from http.server import HTTPServer, BaseHTTPRequestHandler
                          from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer, TrainerCallback
                          from datasets import Dataset

                          # Ensure state.is_world_process_zero works correctly
                          os.environ.setdefault("RANK", "0")

                          print("[E2E Failure Test] Using HuggingFace Transformers (will inject failure)", flush=True)

                          # Global metrics state
                          _metrics = {
                              "progressPercentage": 0,
                              "estimatedRemainingSeconds": None,
                              "currentStep": 0,
                              "totalSteps": None,
                              "currentEpoch": 0.0,
                              "totalEpochs": None,
                              "trainMetrics": {},
                              "evalMetrics": {}
                          }
                          _lock = threading.Lock()

                          class MetricsHandler(BaseHTTPRequestHandler):
                              def do_GET(self):
                                  if self.path == '/metrics':
                                      with _lock:
                                          data = json.dumps(_metrics)
                                      self.send_response(200)
                                      self.send_header('Content-Type', 'application/json')
                                      self.end_headers()
                                      self.wfile.write(data.encode())
                                  else:
                                      self.send_error(404)
                              def log_message(self, *args): pass

                          # Progression callback (with termination message write even on failure)
                          class ProgressCallback(TrainerCallback):
                              def __init__(self):
                                  self.start_time = None
                                  self.server = None
                                  self.training_finished = False

                              def on_train_begin(self, args, state, control, **kwargs):
                                  self.start_time = time.time()

                                  with _lock:
                                      _metrics.update({
                                          "progressPercentage": 0,
                                          "estimatedRemainingSeconds": None,
                                          "currentStep": state.global_step or 0,
                                          "totalSteps": state.max_steps if state.max_steps > 0 else None,
                                          "currentEpoch": 0.0,
                                          "totalEpochs": int(args.num_train_epochs) if hasattr(args, 'num_train_epochs') else None
                                      })

                                  if state.is_world_process_zero:
                                      try:
                                          srv = HTTPServer(('0.0.0.0', 28080), MetricsHandler)
                                          threading.Thread(target=srv.serve_forever, daemon=True).start()
                                          self.server = srv
                                          print("[E2E Failure Test] Metrics server started", flush=True)
                                      except Exception as e:
                                          print(f"[E2E Failure Test] Warning: Failed to start metrics server: {e}", flush=True)

                              def on_step_end(self, args, state, control, **kwargs):
                                  # Don't overwrite completion state if training has already ended
                                  if not self.training_finished:
                                      step = state.global_step or 0
                                      total = state.max_steps
                                      pct = int(step / total * 100) if total > 0 else 0

                                      # Calculate estimated remaining time
                                      estimated_seconds = None
                                      if self.start_time and step > 0:
                                          elapsed = time.time() - self.start_time
                                          steps_per_sec = step / elapsed
                                          remaining_steps = total - step
                                          estimated_seconds = int(remaining_steps / steps_per_sec) if steps_per_sec > 0 else None

                                      with _lock:
                                          _metrics.update({
                                              "progressPercentage": pct,
                                              "currentStep": step,
                                              "totalSteps": total,
                                              "currentEpoch": float(state.epoch) if hasattr(state, 'epoch') and state.epoch else 0.0,
                                              "estimatedRemainingSeconds": estimated_seconds
                                          })

                              def on_log(self, args, state, control, logs=None, **kwargs):
                                  # Capture train metrics
                                  if logs:
                                      with _lock:
                                          train_metrics = {k: v for k, v in logs.items()
                                                          if k in ['loss', 'grad_norm', 'learning_rate', 'train_loss', 'train_runtime']}
                                          if train_metrics:
                                              _metrics["trainMetrics"].update(train_metrics)

                              def on_evaluate(self, args, state, control, metrics=None, **kwargs):
                                  # Capture eval metrics
                                  if metrics:
                                      with _lock:
                                          eval_metrics = {k: v for k, v in metrics.items() if k.startswith('eval_')}
                                          if eval_metrics:
                                              _metrics["evalMetrics"].update(eval_metrics)

                              def on_train_end(self, args, state, control, **kwargs):
                                  # Write termination message (called even on exception by Transformers)
                                  self.training_finished = True

                                  # Calculate actual progress (not forced to 100% on failure)
                                  current_step = state.global_step if state.global_step is not None else 0
                                  total_steps = state.max_steps if state.max_steps > 0 else None
                                  progress_pct = (
                                      int(current_step / total_steps * 100)
                                      if total_steps and total_steps > 0 and current_step >= 0
                                      else 100
                                  )

                                  # Update HTTP server metrics
                                  with _lock:
                                      _metrics.update({
                                          "progressPercentage": progress_pct,
                                          "estimatedRemainingSeconds": 0,
                                          "currentStep": current_step,
                                          "totalSteps": total_steps
                                      })

                                  if state.is_world_process_zero:
                                      try:
                                          with _lock:
                                              msg = {
                                                  "progressPercentage": progress_pct,
                                                  "estimatedRemainingSeconds": 0,
                                                  "currentStep": current_step,
                                                  "totalSteps": total_steps,
                                                  "currentEpoch": float(state.epoch) if hasattr(state, 'epoch') and state.epoch else 0.0,
                                                  "totalEpochs": int(args.num_train_epochs) if hasattr(args, 'num_train_epochs') else None,
                                                  "trainMetrics": dict(_metrics.get("trainMetrics", {})),
                                                  "evalMetrics": dict(_metrics.get("evalMetrics", {}))
                                              }
                                          with open("/dev/termination-log", "w") as f:
                                              json.dump(msg, f)
                                          print(f"[E2E Failure Test] Termination message written: progress={progress_pct}%", flush=True)
                                      except Exception as e:
                                          print(f"[E2E Failure Test] Warning: Failed to write termination message: {e}", flush=True)

                          # Failure injection callback
                          class FailureCallback(TrainerCallback):
                              def on_step_end(self, args, state, control, **kwargs):
                                  if state.global_step >= 3:
                                      print(f"[E2E Failure Test] ERROR: Simulated failure at step {state.global_step}", flush=True)
                                      raise RuntimeError("Simulated training failure")

                          # Quick training with distilbert-base-uncased (lightweight, 66M params)
                          data = Dataset.from_dict({"text": ["test"] * 10, "label": [0] * 10})
                          tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
                          model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
                          data = data.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=32), batched=True)

                          args = TrainingArguments(
                              output_dir="/tmp/test-fail", num_train_epochs=1, per_device_train_batch_size=2,
                              max_steps=10, save_strategy="no", logging_steps=1, report_to="none", disable_tqdm=True
                          )

                          trainer = Trainer(model=model, args=args, train_dataset=data, callbacks=[ProgressCallback(), FailureCallback()])
                          try:
                              trainer.train()
                          except Exception as e:
                              print(f"[E2E Failure Test] Training failed as expected: {e}", flush=True)
                              # Write termination message before exiting
                              state = trainer.state
                              args = trainer.args
                              current_step = state.global_step if state.global_step is not None else 0
                              total_steps = state.max_steps if state.max_steps > 0 else None
                              progress_pct = (
                                  int(current_step / total_steps * 100)
                                  if total_steps and total_steps > 0 and current_step >= 0
                                  else 0
                              )
                              # Update HTTP metrics before exit
                              with _lock:
                                  _metrics.update({
                                      "progressPercentage": progress_pct,
                                      "estimatedRemainingSeconds": 0,
                                      "currentStep": current_step,
                                      "totalSteps": total_steps
                                  })
                              # Write termination message
                              try:
                                  msg = {
                                      "progressPercentage": progress_pct,
                                      "estimatedRemainingSeconds": 0,
                                      "currentStep": current_step,
                                      "totalSteps": total_steps,
                                      "currentEpoch": float(state.epoch) if hasattr(state, 'epoch') and state.epoch else 0.0,
                                      "totalEpochs": int(args.num_train_epochs) if hasattr(args, 'num_train_epochs') else None,
                                      "trainMetrics": {},
                                      "evalMetrics": {}
                                  }
                                  with open("/dev/termination-log", "w") as f:
                                      json.dump(msg, f)
                                  print(f"[E2E Failure Test] Termination message written on failure: progress={progress_pct}%", flush=True)
                              except Exception as term_ex:
                                  print(f"[E2E Failure Test] Warning: Failed to write termination message: {term_ex}", flush=True)
                              sys.exit(1)
                      readinessProbe:
                        httpGet:
                          path: /metrics
                          port: 28080
                        initialDelaySeconds: 2
                        periodSeconds: 1
                        timeoutSeconds: 1
                        failureThreshold: 2
                  restartPolicy: Never
