apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainingRuntime
metadata:
  name: no-metrics-runtime
spec:
  mlPolicy:
    numNodes: 1
    torch:
      numProcPerNode: auto
  template:
    spec:
      replicatedJobs:
        - name: node
          template:
            metadata:
              labels:
                trainer.kubeflow.org/trainjob-ancestor-step: trainer
            spec:
              backoffLimit: 0
              template:
                spec:
                  containers:
                    - name: node
                      image: quay.io/modh/training:py311-cuda124-torch251
                      command:
                        - python
                        - "-c"
                        - |
                          import time
                          import sys
                          import json

                          # Test fallback when HTTP metrics unavailable
                          # Only writes termination message (no HTTP server)
                          print("Starting training WITHOUT metrics server...")

                          # Fast simulation (5 seconds total)
                          total_steps = 25
                          for step in range(total_steps):
                              time.sleep(0.2)
                              if step % 5 == 0:
                                  print(f"Step {step}/{total_steps} (no metrics exposed)")

                          print("Training completed (no metrics were exposed)")

                          # Write final metrics to termination message
                          try:
                              termination_message = {
                                  "progressPercentage": 100,
                                  "estimatedRemainingSeconds": 0,
                                  "currentStep": total_steps,
                                  "totalSteps": total_steps,
                                  "currentEpoch": 1.0,
                                  "totalEpochs": 1,
                                  "trainMetrics": {},
                                  "evalMetrics": {}
                              }
                              with open("/dev/termination-log", "w") as f:
                                  json.dump(termination_message, f)
                              print("Final metrics written to termination message")
                          except Exception as e:
                              print(f"Warning: Failed to write termination message: {e}")

                          sys.exit(0)
                  restartPolicy: Never
